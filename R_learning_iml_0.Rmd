---
title: "R_learning_iml_0"
author: "Hector Hao"
date: "2019/4/25"
output: html_notebook
---


这是链接 [interpretable machine learning](https://christophm.github.io/interpretable-ml-book/) 

*注：原书很精彩。*

# 序章

```{r}
# iml 书作者维护的包
library(iml)

# bike rentals. & risk factors for cervical cancer.
(.packages())
load('./bike.RData')
load('./cervical.RData')
head(bike)
#########################################################
# 对于每个模块，要学习它的用法，它的图表，它的图表的数据#
# 然后是它的解释。                                      #
#########################################################

```

```{r}
head(cervical)
```

关于未来涉及机器学习模型解释的场景设想：见原文。

核心观念&伦理层面的动机：

* 人为什么要信任机器
* 面对机器的决策，人的权益如何保障。防止算法带来的不公平(例如模型的关键因素是种族)。 

关于机器学习算法的重要用法之一:
这个样例为什么会和另一个样例不同。

机器学习模型两个重要要求：
* 指标(准确率等)
* 解释(要让human理解)

对于一些模型，人们不需要严肃的解释，例如电影推荐系统，例如成熟技术ocr等等。

但更多的模型希望有严肃的解释来辅助决策，a correct prediction only partially solves your original problem.
包括经济决策，包括希望对模型debug，等等场景。


解释的要求：
大部分场合，解释不需要面面俱到。根据人的认知能力，选择最重要的1-3条原因进行解释，或选择反常的事情来解释。经典例子：

*如果一个老师每节课都提问，回答不上来会扣分，那么某个学生挂科了，我们会说是因为学生不努力学习。
如果一个老师非常罕见地提问，回答不上来会扣分，那么某个学生挂科了，我们会说是因为他倒霉，老师很反常，突然提问了。*


